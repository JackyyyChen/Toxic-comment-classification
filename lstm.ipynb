{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from torchtext.vocab import Vocab, GloVe\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data import get_tokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    # 大小写\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace(\"#\" , \" \")\n",
    "    text = text.replace(\".\" , \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('http?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('www.[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "    encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "    decode_string = encoded_string.decode()\n",
    "    return decode_string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "global_vectors = GloVe(name='twitter.27B', dim=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\") ## We'll use tokenizer available from PyTorch\n",
    "embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"fuccccccccccck\"), lower_case_backup=True)\n",
    "s1 =  embeddings.tolist()\n",
    "embeddings1 = global_vectors.get_vecs_by_tokens(tokenizer(\"fuck\"), lower_case_backup=True)\n",
    "s2 = embeddings1.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "([[0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0]],\n [[-0.20878000557422638,\n   0.16732999682426453,\n   -0.007849800400435925,\n   0.11349000036716461,\n   -0.6427599787712097,\n   0.2809000015258789,\n   0.4054499864578247,\n   -0.019015999510884285,\n   0.13429999351501465,\n   0.5757799744606018,\n   0.23590999841690063,\n   -0.18035000562667847,\n   -0.27204999327659607,\n   0.02213600091636181,\n   0.12393999844789505,\n   -0.20116999745368958,\n   -0.030572999268770218,\n   0.10560999810695648,\n   -0.4020000100135803,\n   0.26579999923706055,\n   -0.05012800171971321,\n   -0.05308900028467178,\n   0.4758799970149994,\n   0.16000999510288239,\n   0.04409800097346306,\n   0.33921998739242554,\n   0.2995400130748749,\n   0.1859000027179718,\n   -0.3578700125217438,\n   -0.4156399965286255,\n   -0.3573499917984009,\n   -0.16675999760627747,\n   -0.036368999630212784,\n   0.2007099986076355,\n   -0.8691800236701965,\n   0.5196700096130371,\n   -0.17535999417304993,\n   0.38416001200675964,\n   0.3307099938392639,\n   -0.3296999931335449,\n   0.3405799865722656,\n   0.39090999960899353,\n   -0.19975000619888306,\n   -0.3865399956703186,\n   0.30667001008987427,\n   -0.05708000063896179,\n   0.6248000264167786,\n   0.5539199709892273,\n   0.17190000414848328,\n   0.2423499971628189,\n   0.258109986782074,\n   0.25290000438690186,\n   -0.046144999563694,\n   -0.34397000074386597,\n   0.460860013961792,\n   -0.03905399888753891,\n   -0.9905400276184082,\n   -0.05052100121974945,\n   -0.20592999458312988,\n   0.30212000012397766,\n   0.2651999890804291,\n   0.007574399933218956,\n   0.34092000126838684,\n   0.01768600009381771,\n   -0.26554998755455017,\n   -0.25968998670578003,\n   -0.10327000170946121,\n   0.271699994802475,\n   -0.19829000532627106,\n   0.28327998518943787,\n   -0.1987999975681305,\n   0.3883500099182129,\n   -0.3078100085258484,\n   -0.14594000577926636,\n   -0.07777699828147888,\n   0.03445500135421753,\n   0.3745500147342682,\n   0.13676999509334564,\n   0.14956000447273254,\n   0.3537200093269348,\n   0.3718000054359436,\n   0.37167999148368835,\n   -0.6166599988937378,\n   -0.28797999024391174,\n   -0.359609991312027,\n   -0.024112999439239502,\n   -0.36664000153541565,\n   -0.6238600015640259,\n   -0.14544999599456787,\n   -0.19894999265670776,\n   -0.18413999676704407,\n   0.09852900356054306,\n   0.0009160200133919716,\n   0.30792000889778137,\n   0.099372997879982,\n   0.7982000112533569,\n   -0.6392300128936768,\n   -0.030866000801324844,\n   0.7193199992179871,\n   0.3378700017929077,\n   -0.7035300135612488,\n   -0.39239999651908875,\n   0.3852899968624115,\n   -0.4048900008201599,\n   0.3477399945259094,\n   0.1746399998664856,\n   -0.3772999942302704,\n   -0.30803000926971436,\n   0.16437000036239624,\n   -0.16673000156879425,\n   -0.30715999007225037,\n   0.10546000301837921,\n   -0.05650800094008446,\n   -0.594789981842041,\n   -0.44600000977516174,\n   0.06316599994897842,\n   0.1288599967956543,\n   0.032193999737501144,\n   -0.185589998960495,\n   -0.0700559988617897,\n   0.022052999585866928,\n   0.2592400014400482,\n   -0.12752999365329742,\n   0.16606999933719635,\n   -0.045552998781204224,\n   0.41479000449180603,\n   0.34749001264572144,\n   -0.34174999594688416,\n   0.24973000586032867,\n   -0.19818000495433807,\n   0.16086000204086304,\n   -0.13398000597953796,\n   0.29361000657081604,\n   0.09753499925136566,\n   0.08479800075292587,\n   -0.19380000233650208,\n   0.030830999836325645,\n   -0.11384999752044678,\n   0.32649001479148865,\n   -1.0214999914169312,\n   0.06880000233650208,\n   0.32512998580932617,\n   -0.12538999319076538,\n   -0.30722999572753906,\n   -0.05152599886059761,\n   0.4769099950790405,\n   0.09206099808216095,\n   -0.024898000061511993,\n   -0.19458000361919403,\n   0.2327200025320053,\n   -0.025126000866293907,\n   -0.08159200102090836,\n   -5.4721999168396,\n   -0.07450299710035324,\n   0.09279199689626694,\n   -0.47314000129699707,\n   -0.36166998744010925,\n   0.194350004196167,\n   -0.18196000158786774,\n   0.26082998514175415,\n   -0.10469000041484833,\n   -0.2904700040817261,\n   -0.049911998212337494,\n   0.07911600172519684,\n   -0.1335200071334839,\n   0.34836000204086304,\n   0.146029993891716,\n   -0.05588499829173088,\n   -0.05167600139975548,\n   0.1962299942970276,\n   -0.43039000034332275,\n   -0.13455000519752502,\n   0.0906900018453598,\n   0.19047999382019043,\n   -0.2953000068664551,\n   -0.326229989528656,\n   0.23754000663757324,\n   0.11935999989509583,\n   -0.16474999487400055,\n   -0.029905999079346657,\n   -0.42162999510765076,\n   -0.14326000213623047,\n   -0.3177199959754944,\n   0.377020001411438,\n   -0.28053000569343567,\n   -0.006081299856305122,\n   0.12549999356269836,\n   0.052949998527765274,\n   -0.7153699994087219,\n   -0.2144400030374527,\n   -0.546209990978241,\n   -0.28773000836372375,\n   0.11885000020265579,\n   0.00376180000603199,\n   0.26881998777389526,\n   -0.15347999334335327,\n   0.11508999764919281,\n   0.099031001329422,\n   -0.4101699888706207,\n   0.3161199986934662]])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1, s2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n135109  d2b6d3324dc09a60                  ARSE YOU FUCKING PRICK. FUCK OFF.   \n75389   c9b0eba0f9d55fbc  P.S Im a fucking cunt rag and enjoy the taste ...   \n92124   f64b7d5518c05c76                           bitch \\n\\nfuck you b***h   \n27450   48a6c30c8ed77d59  FUCK YOU FUCK YOU FUCK YOU \\n\\nFFFFFFFFUUUUUUU...   \n76433   cca06f093aec503b  FUCK QUEBECFAGGOTS. FUCKING SEPERATE SO WE CAN...   \n38775   677b5c663a0758ae  Hi \\n\\nHi you are gay sad fucking old nerd tha...   \n6       0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n120395  83f66f6c00cf2985  DONT SEND ME A WARNING AGAIN OR I WILL RAPE YO...   \n8619    16e110672dde00d9  How civil is this - who the fuck do you think ...   \n139493  eaa76f9c074e3aa5  Daedalus is nothing but a filthy n!gger and a ...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n135109      1             1        1       0       1              0  \n75389       1             1        1       0       0              0  \n92124       1             1        1       0       1              0  \n27450       1             1        1       0       1              0  \n76433       1             1        1       0       1              0  \n38775       1             1        1       0       1              1  \n6           1             1        1       0       1              0  \n120395      1             1        1       1       0              0  \n8619        1             1        1       0       1              0  \n139493      1             1        1       0       1              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>135109</th>\n      <td>d2b6d3324dc09a60</td>\n      <td>ARSE YOU FUCKING PRICK. FUCK OFF.</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>75389</th>\n      <td>c9b0eba0f9d55fbc</td>\n      <td>P.S Im a fucking cunt rag and enjoy the taste ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>92124</th>\n      <td>f64b7d5518c05c76</td>\n      <td>bitch \\n\\nfuck you b***h</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27450</th>\n      <td>48a6c30c8ed77d59</td>\n      <td>FUCK YOU FUCK YOU FUCK YOU \\n\\nFFFFFFFFUUUUUUU...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>76433</th>\n      <td>cca06f093aec503b</td>\n      <td>FUCK QUEBECFAGGOTS. FUCKING SEPERATE SO WE CAN...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38775</th>\n      <td>677b5c663a0758ae</td>\n      <td>Hi \\n\\nHi you are gay sad fucking old nerd tha...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0002bcb3da6cb337</td>\n      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>120395</th>\n      <td>83f66f6c00cf2985</td>\n      <td>DONT SEND ME A WARNING AGAIN OR I WILL RAPE YO...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8619</th>\n      <td>16e110672dde00d9</td>\n      <td>How civil is this - who the fuck do you think ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>139493</th>\n      <td>eaa76f9c074e3aa5</td>\n      <td>Daedalus is nothing but a filthy n!gger and a ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_path = '/mnt/a/OneDrive/UNSW/COMP9444/9444_toxic_comment_classification/data/'\n",
    "train = pd.read_csv(data_folder_path + 'train.csv')\n",
    "test_data = pd.read_csv(data_folder_path + 'test.csv')\n",
    "test_labels = pd.read_csv(data_folder_path + 'test_labels.csv')\n",
    "test_data = pd.concat([test_data, test_labels], axis=1)\n",
    "\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train[train[classes[1]] == 1].sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "#seed = 42\n",
    "#train, test_data = train_test_split(train, test_size=0.2, random_state=seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic rate:  9.584 %\n",
      "severe_toxic rate:  1.0 %\n",
      "obscene rate:  5.295 %\n",
      "threat rate:  0.3 %\n",
      "insult rate:  4.936 %\n",
      "identity_hate rate:  0.88 %\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    rate = train[cls].sum() / len(train)\n",
    "    rate = np.round(rate*100, 3)\n",
    "    print(cls +' rate: ', rate, \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic rate:  -54.253 %\n",
      "severe_toxic rate:  -57.989 %\n",
      "obscene rate:  -55.819 %\n",
      "threat rate:  -58.091 %\n",
      "insult rate:  -55.992 %\n",
      "identity_hate rate:  -57.764 %\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    rate = test_data[cls].sum() / len(test_data)\n",
    "    rate = np.round(rate*100, 3)\n",
    "    print(cls +' rate: ', rate, \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8331/1009963608.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"comment_text\"].iloc[idx] = global_vectors.get_vecs_by_tokens(token, lower_case_backup=True)\n"
     ]
    }
   ],
   "source": [
    "#l_train = []\n",
    "l_train_drop =[]\n",
    "\n",
    "for idx in range(0, len(train)):\n",
    "    comment = train[\"comment_text\"].iloc[idx]\n",
    "    clean = data_cleaning(comment)\n",
    "    token = tokenizer(clean)\n",
    "    if len(token) == 0:\n",
    "        l_train_drop.append(idx)\n",
    "        continue\n",
    "    train[\"comment_text\"].iloc[idx] = global_vectors.get_vecs_by_tokens(token, lower_case_backup=True)\n",
    "\n",
    "\n",
    "#    row['embeddings'] = embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "train.drop(l_train_drop, axis = 0, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "159566"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in range(0, len(train)):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937, -0.1251, -0.5924,  ...,  0.7811, -0.1109, -0.2980],\n",
      "        [ 0.2539,  0.3569,  0.2210,  ...,  0.1067, -0.1587,  0.1287],\n",
      "        [ 0.4935,  0.3570,  0.6607,  ...,  0.1771, -0.5369, -0.2970],\n",
      "        ...,\n",
      "        [ 0.1430, -0.4178,  0.2529,  ..., -0.7805,  0.3792,  0.4934],\n",
      "        [ 0.1588,  0.2819, -0.4703,  ...,  0.8744, -0.5519, -0.1927],\n",
      "        [-0.0218, -0.0115, -0.4395,  ...,  0.4765,  0.2968,  0.6029]])\n",
      "PackedSequence(data=tensor([-0.1937,  0.2539,  0.4935,  ...,  0.4934, -0.1927,  0.6029]), batch_sizes=tensor([51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.2442,  0.3383,  0.5033,  ..., -0.4469, -0.2331,  0.0522],\n",
      "        [-0.5164,  0.0647, -0.3937,  ..., -0.2190, -0.1617, -0.3781],\n",
      "        [ 0.0677, -0.2674,  0.0728,  ..., -0.4911, -0.2488,  0.6093],\n",
      "        ...,\n",
      "        [ 0.1676, -0.2118,  0.0583,  ..., -0.2236, -0.5680, -0.0579],\n",
      "        [ 0.4132,  0.1357, -0.3712,  ...,  0.5137,  0.7586,  0.2672],\n",
      "        [ 0.3119, -0.0463, -0.1282,  ..., -0.7497, -0.5116,  0.2095]])\n",
      "PackedSequence(data=tensor([ 0.2442, -0.5164,  0.0677,  ..., -0.0579,  0.2672,  0.2095]), batch_sizes=tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.5244,  0.2295, -0.2096,  ..., -0.3582,  0.2474, -0.4814],\n",
      "        [-0.1859,  0.1115, -0.0507,  ...,  0.2287, -0.2876,  0.1388],\n",
      "        [ 0.3927, -0.0842, -0.6075,  ...,  0.0939, -0.5851,  0.1954],\n",
      "        ...,\n",
      "        [ 0.4935,  0.3570,  0.6607,  ...,  0.1771, -0.5369, -0.2970],\n",
      "        [-0.0516, -0.0598, -0.1052,  ...,  0.5716, -0.5702,  0.1067],\n",
      "        [-0.4089,  0.3150, -0.8155,  ...,  0.0263, -0.3000,  0.5652]])\n",
      "PackedSequence(data=tensor([ 0.5244, -0.1859,  0.3927,  ..., -0.2970,  0.1067,  0.5652]), batch_sizes=tensor([47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.2453,  0.0183, -0.3511,  ..., -0.2389, -0.3607,  0.1470],\n",
      "        [ 0.0564,  0.4954,  0.1844,  ...,  0.6360, -0.1888, -0.0356],\n",
      "        [ 0.0494,  0.6521,  0.0852,  ..., -0.2740,  0.3942, -0.1203],\n",
      "        ...,\n",
      "        [-0.0540,  0.1507, -0.7549,  ...,  0.0609, -0.7760, -0.6108],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4382,  0.3922,  0.0022,  ...,  0.4480, -0.2087,  0.0090]])\n",
      "PackedSequence(data=tensor([ 0.2453,  0.0564,  0.0494,  ..., -0.6108,  0.0000,  0.0090]), batch_sizes=tensor([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.1964,  0.6715,  0.0063,  ..., -0.3081, -0.1231,  0.0489],\n",
      "        [ 0.3927, -0.0842, -0.6075,  ...,  0.0939, -0.5851,  0.1954],\n",
      "        [ 0.3607, -0.1164, -0.3302,  ..., -0.1843,  0.5323,  0.3184],\n",
      "        ...,\n",
      "        [ 0.5516, -0.1907,  0.2794,  ..., -0.3413,  0.3175,  0.2361],\n",
      "        [-0.7518, -0.2664,  0.4026,  ..., -0.1010, -0.0022, -0.1353],\n",
      "        [ 0.3621,  0.0846, -0.2436,  ..., -0.6513, -0.7392,  0.5278]])\n",
      "PackedSequence(data=tensor([ 0.1964,  0.3927,  0.3607,  ...,  0.2361, -0.1353,  0.5278]), batch_sizes=tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "\n",
    "for idx in range(0, 5):\n",
    "    comment = train[\"comment_text\"][idx]\n",
    "    print(comment)\n",
    "    pad = pack_sequence(comment)\n",
    "    print(pad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pad.data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([18, 200])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 631626054400 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [91]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m padded \u001B[38;5;241m=\u001B[39m \u001B[43mpack_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcomment_text\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/COMP9444/lib/python3.9/site-packages/torch/nn/utils/rnn.py:482\u001B[0m, in \u001B[0;36mpack_sequence\u001B[0;34m(sequences, enforce_sorted)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Packs a list of variable length Tensors\u001B[39;00m\n\u001B[1;32m    451\u001B[0m \n\u001B[1;32m    452\u001B[0m \u001B[38;5;124;03mConsecutive call of the next functions: ``pad_sequence``, ``pack_padded_sequence``.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;124;03m    a :class:`PackedSequence` object\u001B[39;00m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    481\u001B[0m lengths \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor([v\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m sequences])\n\u001B[0;32m--> 482\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_padded_sequence(\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m)\u001B[49m, lengths, enforce_sorted\u001B[38;5;241m=\u001B[39menforce_sorted)\n",
      "File \u001B[0;32m~/miniconda3/envs/COMP9444/lib/python3.9/site-packages/torch/nn/utils/rnn.py:396\u001B[0m, in \u001B[0;36mpad_sequence\u001B[0;34m(sequences, batch_first, padding_value)\u001B[0m\n\u001B[1;32m    392\u001B[0m         sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    394\u001B[0m \u001B[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001B[39;00m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001B[39;00m\n\u001B[0;32m--> 396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 631626054400 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "padded = pack_sequence(train[\"comment_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}