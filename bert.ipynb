{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yiwei\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "data_folder_path = '//mnt/a/OneDrive/UNSW/COMP9444/9444_toxic_comment_classification/data/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_folder_path + 'train.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n128869  b1271c212b64ca18  Gay ass william at it again small penis fagbag...   \n18100   2fd54fdafc99da95  You swine. You vulgar little maggot. You worth...   \n52394   8c35385171575379  You are a fucking dick pseudomonas \\n\\nyou do ...   \n29445   4e17edc5e38b8335       If you don't let me post, I will fuck you up   \n89046   ee32164a4d102aa7  \"\\n\\nFuck off you stupid fucking retard cunt.\\...   \n1465    03ebb03deb920216  To Blanchardb \\n\\nYou dumb motherfucker I am g...   \n92746   f7fd19e7a6ea2209  \"How dare you filthy stinking whores disrupt m...   \n84948   e3591ced175a97d6  Bottomley \\n\\nIt's not an attack page you dick...   \n154553  af9e849c953035b1  You're gay you're gay you're gay you're gay yo...   \n48618   820493434cb9153e  ASSHOLE! \\n\\nI just read this article, and man...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n128869      1             1        1       0       1              0  \n18100       1             1        1       0       1              0  \n52394       1             1        1       0       0              0  \n29445       1             1        1       1       0              0  \n89046       1             1        1       0       1              0  \n1465        1             1        1       0       1              0  \n92746       1             1        0       0       0              0  \n84948       1             1        1       0       1              0  \n154553      1             1        0       0       1              1  \n48618       1             1        1       0       1              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>128869</th>\n      <td>b1271c212b64ca18</td>\n      <td>Gay ass william at it again small penis fagbag...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18100</th>\n      <td>2fd54fdafc99da95</td>\n      <td>You swine. You vulgar little maggot. You worth...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52394</th>\n      <td>8c35385171575379</td>\n      <td>You are a fucking dick pseudomonas \\n\\nyou do ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29445</th>\n      <td>4e17edc5e38b8335</td>\n      <td>If you don't let me post, I will fuck you up</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>89046</th>\n      <td>ee32164a4d102aa7</td>\n      <td>\"\\n\\nFuck off you stupid fucking retard cunt.\\...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1465</th>\n      <td>03ebb03deb920216</td>\n      <td>To Blanchardb \\n\\nYou dumb motherfucker I am g...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>92746</th>\n      <td>f7fd19e7a6ea2209</td>\n      <td>\"How dare you filthy stinking whores disrupt m...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>84948</th>\n      <td>e3591ced175a97d6</td>\n      <td>Bottomley \\n\\nIt's not an attack page you dick...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>154553</th>\n      <td>af9e849c953035b1</td>\n      <td>You're gay you're gay you're gay you're gay yo...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48618</th>\n      <td>820493434cb9153e</td>\n      <td>ASSHOLE! \\n\\nI just read this article, and man...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[classes[1]] == 1].sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "each comment can have more than one label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(data_folder_path + 'test.csv')\n",
    "test_labels = pd.read_csv(data_folder_path + 'test_labels.csv')\n",
    "test_data = pd.concat([test_data, test_labels], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic rate:  9.584 %\n",
      "severe_toxic rate:  1.0 %\n",
      "obscene rate:  5.295 %\n",
      "threat rate:  0.3 %\n",
      "insult rate:  4.936 %\n",
      "identity_hate rate:  0.88 %\n"
     ]
    }
   ],
   "source": [
    "total_samples = df.shape[0]\n",
    "for cls in classes:\n",
    "    rate = df[cls].sum() / total_samples\n",
    "    rate = np.round(rate*100, 3)\n",
    "    print(cls +' rate: ', rate, \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    # seems that Uppercase words have more effect on toxicity than lowercase.\n",
    "    # so I decided to keep them as they are.\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace(\"#\" , \" \")\n",
    "\n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('http?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('www.[A-Za-z0-9./]+', '', text)\n",
    "    encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "    decode_string = encoded_string.decode()\n",
    "    return decode_string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df['clean_comment'] = df['comment_text'].apply(data_cleaning)\n",
    "test_data['clean_comment'] = test_data['comment_text'].apply(data_cleaning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n140030  ed56f082116dcbd0  Grandma Terri Should Burn in Trash \\nGrandma T...   \n159124  f8e3cd98b63bf401  , 9 May 2009 (UTC)\\nIt would be easiest if you...   \n60006   a09e1bcf10631f9a  \"\\n\\nThe Objectivity of this Discussion is dou...   \n65432   af0ee0066c607eb8              Shelly Shock\\nShelly Shock is. . .( )   \n154979  b734772b1a807e09  I do not care. Refer to Ong Teng Cheong talk p...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n140030      1             0        0       0       0              0   \n159124      0             0        0       0       0              0   \n60006       0             0        0       0       0              0   \n65432       0             0        0       0       0              0   \n154979      0             0        0       0       0              0   \n\n                                            clean_comment  \n140030  Grandma Terri Should Burn in Trash  Grandma Te...  \n159124  , 9 May 2009 (UTC) It would be easiest if you ...  \n60006   \"  The Objectivity of this Discussion is doubt...  \n65432                Shelly Shock Shelly Shock is. . .( )  \n154979  I do not care. Refer to Ong Teng Cheong talk p...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>clean_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>140030</th>\n      <td>ed56f082116dcbd0</td>\n      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Grandma Terri Should Burn in Trash  Grandma Te...</td>\n    </tr>\n    <tr>\n      <th>159124</th>\n      <td>f8e3cd98b63bf401</td>\n      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>, 9 May 2009 (UTC) It would be easiest if you ...</td>\n    </tr>\n    <tr>\n      <th>60006</th>\n      <td>a09e1bcf10631f9a</td>\n      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>\"  The Objectivity of this Discussion is doubt...</td>\n    </tr>\n    <tr>\n      <th>65432</th>\n      <td>af0ee0066c607eb8</td>\n      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Shelly Shock Shelly Shock is. . .( )</td>\n    </tr>\n    <tr>\n      <th>154979</th>\n      <td>b734772b1a807e09</td>\n      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# encoded_comment = [tokenizer.encode(sent, add_special_tokens=True) for sent in train_df['clean_comment']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# comment_len = [len(x) for x in encoded_comment]\n",
    "# np.max(comment_len), np.quantile(comment_len, 0.97), np.mean(comment_len), np.median(comment_len), np.min(comment_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "97% of the comments are less than 436 tokens, and longer comments always tend to be non-toxic. so I decided to use max_len = 436."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "MAX_LEN = 436"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class BertDataSet(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.comments = dataframe['clean_comment'].values\n",
    "        self.labels = dataframe[classes].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.comments[idx]\n",
    "        tokenized_comment = tokenizer.encode_plus(comment,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    max_length = MAX_LEN,\n",
    "                                                    padding='max_length',\n",
    "                                                    truncation = True,\n",
    "                                                    return_attention_mask = True)\n",
    "        ids = torch.tensor(tokenized_comment['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(tokenized_comment['attention_mask'], dtype=torch.long)\n",
    "\n",
    "        labels = self.labels[idx]\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return {'ids': ids, 'mask': mask, 'labels': labels}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "dataset_train = BertDataSet(train_df)\n",
    "dataset_test = BertDataSet(valid_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(2000, 1000)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([436]) torch.Size([436]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for td in dataset_test:\n",
    "    print(td['ids'].shape, td['mask'].shape, td['labels'].shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_batch = 48\n",
    "test_batch = 48"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset_train, batch_size=train_batch, shuffle=True, pin_memory = True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=test_batch, shuffle=False, pin_memory = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 3 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataParallel(\n  (module): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=6, bias=True)\n  )\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 6)\n",
    "gpus = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if gpus > 1:\n",
    "    print(\"Let's use\", gpus, \"GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)    # multi-gpu\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "loss.to(device)\n",
    "for batch in data_loader_train:\n",
    "    ids = batch['ids'].to(device)\n",
    "    mask = batch['mask'].to(device)\n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    outputs = outputs['logits'].squeeze(-1).to(torch.float32)\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predictions = torch.where(probabilities > 0.5, 1, 0)\n",
    "    labels = batch['labels'].to(device, non_blocking=True)\n",
    "    loss_value = loss(outputs, labels)\n",
    "    print(loss_value.item())\n",
    "    correct_predictions = torch.sum(predictions == labels)\n",
    "    print(correct_predictions.item())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "loss.to(device)\n",
    "epochs = 5\n",
    "LR = 2e-5 #Learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR, weight_decay = 1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 2, verbose = True)\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.8139429092407227\n",
      "Epoch: 0, Batch: 10, Loss: 0.520084798336029\n",
      "Epoch: 0, Batch: 20, Loss: 0.33398282527923584\n",
      "Epoch: 0, Batch: 30, Loss: 0.1946413218975067\n",
      "Epoch: 0, Batch: 40, Loss: 0.19009457528591156\n",
      "Epoch: 0, Accuracy: 0.8760833144187927\n",
      "Epoch: 0, Validation Accuracy: 0.9678333401679993, loss: 0.15576065154302687\n",
      "Epoch: 1, Batch: 0, Loss: 0.1112305074930191\n",
      "Epoch: 1, Batch: 10, Loss: 0.213715597987175\n",
      "Epoch: 1, Batch: 20, Loss: 0.16956229507923126\n",
      "Epoch: 1, Batch: 30, Loss: 0.1453367918729782\n",
      "Epoch: 1, Batch: 40, Loss: 0.06897848099470139\n",
      "Epoch: 1, Accuracy: 0.9604166746139526\n",
      "Epoch: 1, Validation Accuracy: 0.9678333401679993, loss: 0.10331601010901588\n",
      "Epoch: 2, Batch: 0, Loss: 0.16576775908470154\n",
      "Epoch: 2, Batch: 10, Loss: 0.08768464624881744\n",
      "Epoch: 2, Batch: 20, Loss: 0.07879272103309631\n",
      "Epoch: 2, Batch: 30, Loss: 0.08739092946052551\n",
      "Epoch: 2, Batch: 40, Loss: 0.06391213089227676\n",
      "Epoch: 2, Accuracy: 0.9704999923706055\n",
      "Epoch: 2, Validation Accuracy: 0.9798333048820496, loss: 0.08179480724391483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    correct_predictions = 0\n",
    "    for batch_id, batch in enumerate(data_loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        train_losses = []\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ids = batch['ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ids, mask)\n",
    "            outputs = outputs['logits'].squeeze(-1).to(torch.float32)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = torch.where(probabilities > 0.5, 1, 0)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "            loss_value = loss(outputs, labels)\n",
    "            train_losses.append(loss_value.item())\n",
    "            loss_value.backward()\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "        optimizer.step()\n",
    "        if batch_id % 10 == 0:\n",
    "            print('Epoch: {}, Batch: {}, Loss: {}'.format(i, batch_id, np.mean(train_losses)))\n",
    "    accuracy = correct_predictions/(len(dataset_train)*6)\n",
    "    print('Epoch: {}, Accuracy: {}'.format(i, accuracy))\n",
    "    model.eval()\n",
    "    # test\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        test_losses = []\n",
    "        for batch_id, batch in enumerate(data_loader_test):\n",
    "            ids = batch['ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            outputs = outputs['logits'].squeeze(-1).to(torch.float32)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = torch.where(probabilities > 0.5, 1, 0)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "            loss_valid = loss(outputs, labels)\n",
    "            test_losses.append(loss_valid.item())\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "        accuracy = correct_predictions/(len(dataset_test)*6)\n",
    "        print('Epoch: {}, Validation Accuracy: {}, loss: {}'.format(i, accuracy, np.mean(test_losses)))\n",
    "        if accuracy > 0.97:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |    6352 MB |    4633 GB |    4633 GB |\n",
      "|       from large pool |       0 B  |    6351 MB |    4619 GB |    4619 GB |\n",
      "|       from small pool |       0 B  |       2 MB |      14 GB |      14 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |    6352 MB |    4633 GB |    4633 GB |\n",
      "|       from large pool |       0 B  |    6351 MB |    4619 GB |    4619 GB |\n",
      "|       from small pool |       0 B  |       2 MB |      14 GB |      14 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6532 MB |    6532 MB |    6532 MB |       0 B  |\n",
      "|       from large pool |    6528 MB |    6528 MB |    6528 MB |       0 B  |\n",
      "|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    2433 MB |    2473 GB |    2473 GB |\n",
      "|       from large pool |       0 B  |    2431 MB |    2458 GB |    2458 GB |\n",
      "|       from small pool |       0 B  |       1 MB |      14 GB |      14 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |     367    |  246645    |  246645    |\n",
      "|       from large pool |       0    |     308    |  163422    |  163422    |\n",
      "|       from small pool |       0    |     137    |   83223    |   83223    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |     367    |  246645    |  246645    |\n",
      "|       from large pool |       0    |     308    |  163422    |  163422    |\n",
      "|       from small pool |       0    |     137    |   83223    |   83223    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      94    |      94    |      94    |       0    |\n",
      "|       from large pool |      92    |      92    |      92    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      73    |  137339    |  137339    |\n",
      "|       from large pool |       0    |      61    |  107414    |  107414    |\n",
      "|       from small pool |       0    |      16    |   29925    |   29925    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}